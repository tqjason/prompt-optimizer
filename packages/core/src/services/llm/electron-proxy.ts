import { ILLMService, Message, StreamHandlers, LLMResponse, ModelOption, ToolDefinition } from './types';
import { safeSerializeForIPC } from '../../utils/ipc-serialization';
import { InitializationError } from './errors';

/**
 * Electronç¯å¢ƒä¸‹çš„LLMæœåŠ¡ä»£ç†
 * é€šè¿‡IPCè°ƒç”¨ä¸»è¿›ç¨‹ä¸­çš„çœŸå®LLMServiceå®ä¾‹
 */
export class ElectronLLMProxy implements ILLMService {
  private electronAPI: NonNullable<Window['electronAPI']>;

  constructor() {
    // éªŒè¯Electronç¯å¢ƒ
    if (typeof window === 'undefined' || !window.electronAPI) {
      throw new InitializationError('ElectronLLMProxy can only be used in Electron renderer process');
    }
    this.electronAPI = window.electronAPI;
  }

  async testConnection(provider: string): Promise<void> {
    await this.electronAPI.llm.testConnection(provider);
  }

  async sendMessage(messages: Message[], provider: string): Promise<string> {
    // è‡ªåŠ¨åºåˆ—åŒ–ï¼Œé˜²æ­¢Vueå“åº”å¼å¯¹è±¡IPCä¼ é€’é”™è¯¯
    const safeMessages = safeSerializeForIPC(messages);
    return this.electronAPI.llm.sendMessage(safeMessages, provider);
  }

  async sendMessageStructured(messages: Message[], provider: string): Promise<LLMResponse> {
    // è‡ªåŠ¨åºåˆ—åŒ–ï¼Œé˜²æ­¢Vueå“åº”å¼å¯¹è±¡IPCä¼ é€’é”™è¯¯
    const safeMessages = safeSerializeForIPC(messages);
    return this.electronAPI.llm.sendMessageStructured(safeMessages, provider);
  }

  async sendMessageStream(
    messages: Message[],
    provider: string,
    callbacks: StreamHandlers
  ): Promise<void> {
    // è‡ªåŠ¨åºåˆ—åŒ–ï¼Œé˜²æ­¢Vueå“åº”å¼å¯¹è±¡IPCä¼ é€’é”™è¯¯
    const safeMessages = safeSerializeForIPC(messages);

    // é€‚é…å›è°ƒæ¥å£ï¼šStreamHandlers ä½¿ç”¨ onTokenï¼Œè€Œ preload æœŸæœ›çš„æ˜¯ onContent
    const adaptedCallbacks = {
      onContent: callbacks.onToken,  // æ˜ å°„ onToken -> onContent
      onThinking: callbacks.onReasoningToken || (() => {}),  // æ˜ å°„æ¨ç†æµ
      onFinish: () => callbacks.onComplete(),  // æ˜ å°„å®Œæˆå›è°ƒ
      onError: callbacks.onError
    };

    await this.electronAPI.llm.sendMessageStream(safeMessages, provider, adaptedCallbacks);
  }

  async sendMessageStreamWithTools(
    messages: Message[],
    provider: string,
    tools: ToolDefinition[],
    callbacks: StreamHandlers
  ): Promise<void> {
    // è‡ªåŠ¨åºåˆ—åŒ–ï¼Œé˜²æ­¢Vueå“åº”å¼å¯¹è±¡IPCä¼ é€’é”™è¯¯
    const safeMessages = safeSerializeForIPC(messages);
    const safeTools = safeSerializeForIPC(tools);

    // é€‚é…å›è°ƒæ¥å£ï¼šStreamHandlers ä½¿ç”¨ onToken/onToolCallï¼Œè€Œ preload æœŸæœ›ç›¸åº”çš„å›è°ƒ
    const adaptedCallbacks = {
      onContent: callbacks.onToken,  // æ˜ å°„ onToken -> onContent
      onThinking: callbacks.onReasoningToken || (() => {}),  // æ˜ å°„æ¨ç†æµ
      onToolCall: callbacks.onToolCall || (() => {}),  // ğŸ†• æ˜ å°„å·¥å…·è°ƒç”¨å›è°ƒ
      onFinish: () => callbacks.onComplete(),  // æ˜ å°„å®Œæˆå›è°ƒ
      onError: callbacks.onError
    };

    // Prefer the dedicated tools-capable streaming channel when available.
    const maybe = this.electronAPI.llm.sendMessageStreamWithTools;
    if (typeof maybe === 'function') {
      await maybe(safeMessages, provider, safeTools, adaptedCallbacks);
      return;
    }

    // Back-compat fallback (older preload/main): stream without tool-call events.
    await this.electronAPI.llm.sendMessageStream(safeMessages, provider, adaptedCallbacks);
  }

  async fetchModelList(
    provider: string,
    customConfig?: Partial<any>
  ): Promise<ModelOption[]> {
    // è‡ªåŠ¨åºåˆ—åŒ–ï¼Œé˜²æ­¢Vueå“åº”å¼å¯¹è±¡IPCä¼ é€’é”™è¯¯
    const safeCustomConfig = customConfig ? safeSerializeForIPC(customConfig) : customConfig;
    return this.electronAPI.llm.fetchModelList(provider, safeCustomConfig);
  }
} 
